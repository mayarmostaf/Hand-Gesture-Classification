{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2258350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a88d808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_record(record):\n",
    "    # Translate\n",
    "    record[Xs] -= record[Xs[0]]\n",
    "    record[Ys] -= record[Ys[0]]\n",
    "\n",
    "    # Scale\n",
    "    record[Xs] /= (max(record[Xs]) - min(record[Xs]))\n",
    "    record[Ys] /= (max(record[Ys]) - min(record[Ys]))\n",
    "\n",
    "    # Rotate\n",
    "    theta = np.arctan2(record[Xs[9]], record[Ys[9]])\n",
    "    cos = np.cos(theta)\n",
    "    sin = np.sin(theta)\n",
    "\n",
    "    R = np.array([\n",
    "        [cos, -sin],\n",
    "        [sin, cos]\n",
    "    ])\n",
    "\n",
    "    rotated_points = R @ np.vstack((record[Xs], record[Ys]))\n",
    "    record[Xs], record[Ys] = rotated_points[0], rotated_points[1]\n",
    "\n",
    "    # Adjust Xs sign based on the difference\n",
    "    record[Xs] = np.sign(record[Xs[5]] - record[Xs[9]]) * record[Xs]\n",
    "\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef27e96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved models\n",
    "loaded_model = joblib.load('model_variations/best_xgb_model.pkl')\n",
    "loaded_target_encoder = joblib.load('preprocessing_models/target_encoder.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17379d22",
   "metadata": {},
   "source": [
    "#####   Try on Recorded Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e9b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "mycols = [item for i in range(1, 22) for item in (f\"x{i}\", f\"y{i}\")]\n",
    "Xs = [\"x\" + str(i) for i in range(1, 22)]\n",
    "Ys = [\"y\" + str(i) for i in range(1, 22)]\n",
    "\n",
    "video_path = 'video_trial/my_video.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# the video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # You can also use 'XVID' or 'MJPG'\n",
    "out = cv2.VideoWriter('video_trial/output.mp4', fourcc, fps, (frame_height, frame_width))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Rotate the frame to the left (90 degrees counterclockwise)\n",
    "    rotated_frame = cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "\n",
    "    image = rotated_frame\n",
    "    image_height, image_width, _ = image.shape\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    hand_landmarks = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(image, landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            for landmark in landmarks.landmark:\n",
    "                x_px = int(landmark.x * image_width)\n",
    "                y_px = int(landmark.y * image_height)\n",
    "                hand_landmarks.extend([x_px, y_px])\n",
    "\n",
    "    # Check if hand_landmarks is empty\n",
    "    if hand_landmarks:\n",
    "        # Assuming you have a function transform_record defined\n",
    "        record = pd.DataFrame(np.array(hand_landmarks).reshape(1, -1), columns=mycols)\n",
    "        record = record.apply(transform_record, axis=1)\n",
    "        y_pred = loaded_model.predict(record)\n",
    "        label_pred = loaded_target_encoder.inverse_transform(y_pred)[0]\n",
    "\n",
    "        position = (10, 30)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 1\n",
    "        color = (0, 255, 0)\n",
    "        thickness = 2\n",
    "        cv2.putText(image, label_pred, position, font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "    resized_image = cv2.resize(image, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA)\n",
    "    cv2.imshow(\"Hand Landmark Detection\", resized_image)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    out.write(resized_image)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0c465c",
   "metadata": {},
   "source": [
    "#### Live Stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815713df",
   "metadata": {},
   "source": [
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "mycols = [item for i in range(1, 22) for item in (f\"x{i}\", f\"y{i}\")]\n",
    "Xs=[\"x\"+str(i) for i in range(1,22)]\n",
    "Ys=[\"y\"+str(i) for i in range(1,22)]\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    image = frame\n",
    "    image_height, image_width, _ = image.shape\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(image, landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            hand_landmarks = []\n",
    "            for landmark in landmarks.landmark:\n",
    "                x_px = int(landmark.x * image_width)\n",
    "                y_px = int(landmark.y * image_height)\n",
    "                hand_landmarks.extend([x_px, y_px])\n",
    "\n",
    "    record = pd.DataFrame(np.array(hand_landmarks).reshape(1, -1), columns=mycols)\n",
    "    record = record.apply(transform_record, axis=1)\n",
    "    y_pred = loaded_model.predict(record)\n",
    "    label_pred = loaded_target_encoder.inverse_transform(y_pred)[0]\n",
    "\n",
    "    position = (10, 30)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 1\n",
    "    color = (0, 255, 0)\n",
    "    thickness = 2\n",
    "    cv2.putText(image, label_pred, position, font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "    resized_image = cv2.resize(image, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA)\n",
    "    cv2.imshow(\"Hand Landmark Detection\", resized_image)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3233d3",
   "metadata": {},
   "source": [
    "#### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "110bbfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "mycols = [item for i in range(1, 22) for item in (f\"x{i}\", f\"y{i}\")]\n",
    "Xs=[\"x\"+str(i) for i in range(1,22)]\n",
    "Ys=[\"y\"+str(i) for i in range(1,22)]\n",
    "\n",
    "images_folder = 'images_trial'\n",
    "image_files = [f for f in os.listdir(images_folder) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "current_index = 0\n",
    "while True:\n",
    "    if current_index >= len(image_files):\n",
    "        print(\"No more images to process.\")\n",
    "        break\n",
    "\n",
    "    image_path = os.path.join(images_folder, image_files[current_index])\n",
    "    image = cv2.imread(image_path)\n",
    "    image_height, image_width, _ = image.shape\n",
    "\n",
    "    if image is None:\n",
    "        print(f\"Error reading image {image_path}\")\n",
    "        continue\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(image, landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            hand_landmarks = []\n",
    "            for landmark in landmarks.landmark:\n",
    "                x_px = int(landmark.x * image_width)\n",
    "                y_px = int(landmark.y * image_height)\n",
    "                hand_landmarks.extend([x_px, y_px])\n",
    "\n",
    "    record = pd.DataFrame(np.array(hand_landmarks).reshape(1, -1), columns=mycols)\n",
    "    record = record.apply(transform_record, axis=1)\n",
    "    y_pred = loaded_model.predict(record)\n",
    "    label_pred = loaded_target_encoder.inverse_transform(y_pred)[0]\n",
    "    \n",
    "    # Write Class\n",
    "    position = (10, 30)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 1\n",
    "    color = (0, 255, 0)\n",
    "    thickness = 2\n",
    "    cv2.putText(image, label_pred, position, font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "    \n",
    "    # Show the resized image\n",
    "    resized_image = cv2.resize(image, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA)\n",
    "    cv2.imshow(\"Hand Landmark Detection\", resized_image)\n",
    "    \n",
    "    # Wait for key press\n",
    "    key = cv2.waitKey(0) & 0xFF\n",
    "\n",
    "    if key == ord('n'):  # Go to the next image on pressing 'n'\n",
    "        current_index += 1\n",
    "    elif key == ord('q'):  # Quit the loop on pressing 'q'\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
